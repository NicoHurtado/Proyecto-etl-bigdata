from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lag, lead
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
import sys

def create_spark_session(app_name="WeatherAnalysis"):
    print("Initializing Spark session...")
    spark = SparkSession.builder.appName(app_name).getOrCreate()
    return spark

def load_and_prepare_initial_data(spark, input_path):
    print(f"Loading data from {input_path}...")
    df = spark.read.parquet(input_path)    
    columns_for_analysis = ["time", "precipitation_sum", "temperature_2m_max"]
    df_prepared = df.select(*columns_for_analysis).na.drop()
    df_prepared = df_prepared.withColumn("time", col("time").cast("date"))
    print("Data loaded and initial preparation complete.")
    df_prepared.printSchema()
    return df_prepared

def engineer_temporal_features(df, lag_days=3):
    print(f"Engineering temporal features: creating next day precipitation and {lag_days}-day lagged features...")
    
    window_spec = Window.orderBy("time")

    df_with_target = df.withColumn("next_day_precipitation", lead("precipitation_sum", 1).over(window_spec))

    for i in range(1, lag_days + 1):
        df_with_target = df_with_target.withColumn(f"precipitation_lag_{i}", lag("precipitation_sum", i).over(window_spec))
    
    for i in range(1, lag_days + 1):
        df_with_target = df_with_target.withColumn(f"temperature_lag_{i}", lag("temperature_2m_max", i).over(window_spec))

    # Drop rows with nulls generated by lag/lead operations (at the beginning/end of the dataset)
    df_final = df_with_target.na.drop()
    
    print("Temporal feature engineering complete.")
    df_final.printSchema()
    df_final.show(5)
    return df_final

def perform_descriptive_analysis(df):

    print("\nPerforming descriptive analysis...")
    if df.isEmpty():
        print("DataFrame is empty. Skipping descriptive analysis.")
        return

    for column_name in df.columns:
        print(f"\nDescriptive statistics for column: {column_name}")
        df.select(col(column_name).cast("float")).describe().show() # Cast to float for numeric stats
    print("Descriptive analysis complete.\n")

def prepare_data_for_ml(df, label_col_name="next_day_precipitation", feature_cols_to_keep=None):
    
    print(f"Preparing data for ML: setting label to '{label_col_name}'...")
    
    if feature_cols_to_keep:
        columns_to_select = feature_cols_to_keep + [label_col_name]
        df_ml = df.select(*columns_to_select)
    else:
        df_ml = df 

    df_ml = df_ml.withColumnRenamed(label_col_name, "label")
    
    print("ML data preparation complete.")
    df_ml.printSchema()
    return df_ml

def engineer_features(df, input_cols, output_col="features"):
    actual_input_cols = [col_name for col_name in input_cols if col_name != "label"]
    print(f"Engineering features: assembling {actual_input_cols} into vector column '{output_col}'...")
    assembler = VectorAssembler(inputCols=actual_input_cols, outputCol=output_col, handleInvalid="skip")
    df_vectorized = assembler.transform(df)
    print("Feature engineering complete.")
    df_vectorized.printSchema()
    return df_vectorized

def train_linear_regression_model(df, features_col="features", label_col="label"):
    print(f"Training Linear Regression model with features '{features_col}' and label '{label_col}'...")
    lr = LinearRegression(featuresCol=features_col, labelCol=label_col)
    model = lr.fit(df)
    print("Linear Regression model training complete.")
    return model

def make_predictions(model, df):
    print("Making predictions...")
    predictions_df = model.transform(df)
    print("Predictions complete.")
    predictions_df.select("features", "label", "prediction").show(5)
    return predictions_df

def save_predictions(predictions_df, output_path, selected_cols=["features", "label", "prediction"]):
    print(f"Saving predictions ({selected_cols}) to {output_path}...")
    predictions_df.select(*selected_cols) \
                  .write.mode("overwrite") \
                  .json(output_path)
    print(f"Predictions saved successfully to {output_path}.")

def main(input_s3_path, predictions_s3_output_path):
    """Main function to orchestrate the weather analysis pipeline."""
    spark = create_spark_session()

    # 1. Load and prepare initial data
    raw_df = load_and_prepare_initial_data(spark, input_s3_path)

    if raw_df.isEmpty():
        print("No data available after loading and initial preparation. Exiting.")
        spark.stop()
        return

    # 2. Engineer temporal features (target variable and lagged features)
    temporal_df = engineer_temporal_features(raw_df, lag_days=3) # Using 3 lag days as an example

    if temporal_df.isEmpty():
        print("No data available after temporal feature engineering. Exiting.")
        spark.stop()
        return
        
    perform_descriptive_analysis(temporal_df) # Perform analysis on data with new features

    # 3. Define feature columns for the model (excluding original time and precipitation_sum)
    feature_columns = [col_name for col_name in temporal_df.columns if col_name not in ["time", "precipitation_sum", "next_day_precipitation"]]
    
    # 4. Prepare data for ML (select features and rename label)
    ml_ready_df = prepare_data_for_ml(temporal_df, 
                                      label_col_name="next_day_precipitation",
                                      feature_cols_to_keep=feature_columns)

    # 5. Engineer features (assemble feature vector)
    assembler_input_cols = [col_name for col_name in ml_ready_df.columns if col_name != "label"]
    vectorized_df = engineer_features(ml_ready_df, input_cols=assembler_input_cols, output_col="features")

    # 6. Train the model
    model = train_linear_regression_model(vectorized_df, features_col="features", label_col="label")

    # 7. Make predictions
    predictions_result_df = make_predictions(model, vectorized_df)

    # 8. Save predictions
    output_columns = ["time"] + feature_columns + ["label", "prediction"] 
    
    final_output_df = predictions_result_df.select(
        col("time"), 
        col("temperature_2m_max"), # current temp
        col("precipitation_lag_1"),
        col("precipitation_lag_2"),
        col("precipitation_lag_3"),
        col("temperature_lag_1"),
        col("temperature_lag_2"),
        col("temperature_lag_3"),
        col("label").alias("actual_next_day_precipitation"), # "label" is next_day_precipitation
        col("prediction").alias("predicted_next_day_precipitation")
    )

    save_predictions(final_output_df, predictions_s3_output_path, selected_cols=final_output_df.columns)

    print("Weather analysis and prediction pipeline finished successfully.")
    spark.stop()

if __name__ == "__main__":
    DEFAULT_INPUT_PATH = "s3://weather-etl-trusted-nicojaco/processed_weather/"
    DEFAULT_PREDICTIONS_OUTPUT_PATH = "s3://weather-etl-refined-nicojaco/weather_predictions/"

    input_path = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_INPUT_PATH
    predictions_output_path = sys.argv[2] if len(sys.argv) > 2 else DEFAULT_PREDICTIONS_OUTPUT_PATH
    
    print(f"Running weather analysis with Input: {input_path}, Output: {predictions_output_path}")
    
    main(input_path, predictions_output_path)
